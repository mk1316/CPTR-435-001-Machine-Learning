{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k42i8owjt2ke"
   },
   "source": [
    "**CPTR** 435 Machine Learning \n",
    "\n",
    "\n",
    "Name:Kaleb Tsegaye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YQg8IgEt2kf"
   },
   "source": [
    "This activity is adapted from the notebook provided for chapter 2 of *Hands-On Machine Learning with Scikit-Learn & TensorFlow 2nd ed* by Geron (2019).\n",
    "\n",
    "For the original notebook and all other code/data from the book, see:\n",
    "https://github.com/ageron/handson-ml2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLZ8CK0kt2kh"
   },
   "source": [
    "# End-to-end Machine Learning project (Part I: Exploring the data set)\n",
    "\n",
    "The purpose of this activity is to understand the workflow of a machine learning project from start to finish. The specific task and ML algorithms we see in this notebook are not as important as understanding the process that we go through to approach the problem. In the future, you will see that this process translates well to new problems and ML algorithms.\n",
    "\n",
    "## Problem: Predict house prices\n",
    "\n",
    "Suppose you are a data scientist working for a real estate company. Your task is to predict median house values in Californian districts, given a number of features from these districts.\n",
    "\n",
    "The main steps you will go through are:\n",
    "1. Learn about the problem\n",
    "2. Get the data (and examine its structure)\n",
    "3. Create a test set\n",
    "4. Explore and visualize the training set\n",
    "5. Prepare and clean the data for ML algorithms\n",
    "6. Select/develop an ML algorithm\n",
    "7. Tune the approach\n",
    "8. Evaluate trained model\n",
    "9. Deploy the resulting ML system\n",
    "\n",
    "\n",
    "The data set is based on the 1990 California census data. For pupose of the example, the book author (Geron) added a categorical attribute and removed some features. \n",
    "\n",
    "An *input* instance in this problem is a *block group* (refered to as a *district* in the book). A block group has a population of 600 to 3000 people. The *output* is the *median house price* for the *block group* (district).\n",
    "\n",
    "## Classification vs. Regression\n",
    "\n",
    "While the Iris problem was a *classification* problem (predict species for given iris), here the predicted output is not a class label, but median house value (output) for a given district (input). Since the range of the median house value is continuous, not a discrete class assignment, our task today is a *regression* problem.\n",
    "\n",
    "## Supervised vs. Unsupervised\n",
    "\n",
    "Since our data has the correct median house values (output) for each district, this problem is a *supervised learning* problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Wf2dB-3t2ki"
   },
   "source": [
    "**Note (from Geron)**: You may find little differences between the code outputs in the book and in these Jupyter notebooks: these slight differences are mostly due to the random nature of many training algorithms: although I have tried to make these notebooks' outputs as constant as possible, it is impossible to guarantee that they will produce the exact same output on every platform. Also, some data structures (such as dictionaries) do not preserve the item order. Finally, I fixed a few minor bugs (I added notes next to the concerned cells) which lead to slightly different results, without changing the ideas presented in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kmVMOmet2ki"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "815o895Pt2kj"
   },
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yh-V0Oqmt2kk"
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "#IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjEGFfCtt2kn"
   },
   "source": [
    "# Get the data\n",
    "\n",
    "While the Iris data set is conveniently included in ML libraries such as Scikit-Learn, most data we want to work with is not. With real world applications of ML, our data must be loaded from external files. In some cases the data may be in formats that are easy to read into our programs (e.g. XML, JSON, CSV, SQL). In other cases, there may be considerable preprocessing involved to first prepare the data so it may be loaded into our ML systems.\n",
    "\n",
    "Our data set today will be downloaded from the course instructor's github account and saved locally in a ``datasets`` subdirectory. Here we create a function to download and extract the data. \n",
    "\n",
    "For efficiency, ``urlretrieve`` only downloads the file if has not already been downloaded. If it sees the file already at the destination path on your computer, it will not download the file again. \n",
    "\n",
    "https://docs.python.org/3.0/library/urllib.request.html#urllib.request.urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvEnbRHcAOZk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from six.moves import urllib\n",
    "\n",
    "# URL for data file\n",
    "DOWNLOAD_URL = \"https://raw.githubusercontent.com/ackleywill/CPTR435/main/housing.csv\"\n",
    "# https://www.kaggle.com/datasets/camnugent/california-housing-prices\n",
    "\n",
    "# local path where data file will be stored on computer (or in virtual environment)\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "\n",
    "def fetch_housing_data(housing_url=DOWNLOAD_URL, housing_path=HOUSING_PATH):\n",
    "    # create local directories for storing data files (if necessary)\n",
    "    # NOTE: if running this in Colaboratory, these directories will not be\n",
    "    # created on your computer, but in the virtual environment for the notebook\n",
    "    # in colaboratory. It will only be available to this notebook, not others.\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "\n",
    "    # build local path for data file\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    # download datafile if not already downloaded\n",
    "    urllib.request.urlretrieve(housing_url, csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xmgMGVHt2kq"
   },
   "source": [
    "Actually call our new function to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dq8dK-oFt2kr"
   },
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNHWinGTt2kt"
   },
   "source": [
    "## Looking at how the data is structured\n",
    "\n",
    "Before we start working with our data set(s), we want to learn how the data is structured, both its file format and the data structure it is read into.\n",
    "\n",
    "The data file ``housing/housing.csv`` is a comma separated value text file. This is a common text-based file format. You can view the file with a text editor or load it into Excel. \n",
    "\n",
    "We'll let a panda do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcNzQMUqt2ku"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QO4ELA-Vt2kx"
   },
   "source": [
    "Now we call our new function to load the data into our notebook using pandas and display the first 5 rows of our DataFrame (``head()``). This will show us what types of features we are working with in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmqubJfgt2ky"
   },
   "outputs": [],
   "source": [
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kF37RQFqt2k2"
   },
   "source": [
    "### Characteristics of our data\n",
    "\n",
    "Pandas provides several functions for describing the nature of our data set. These include the data types used to store the values, and statistics about the data set.\n",
    "\n",
    "For a description of our pandas DataFrame, we use the function ``info()``. This will tell us the number of rows (entries), number of columns, and the data type for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c98J0wGtt2k3"
   },
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfTAouIht2k7"
   },
   "source": [
    "There are 20,640 instances (districts) in our data set. While this is not a large data set compared with those used in production machine learning systems, it is much larger than our Iris data set (150 instances).\n",
    "\n",
    "\n",
    "To see the number of times each unique value appears in a given column, we use the function ``value_counts()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5l1dnYtt2k8"
   },
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXpDBFi8t2k_"
   },
   "source": [
    "The function ``describe()`` displays various statistics about each column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pt5sE_Dt2lA"
   },
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIZQIcHRt2lE"
   },
   "source": [
    "We can use matplotlib to plot histograms for each column in our data set. The histograms are computed with the pandas ``hist`` function.\n",
    "\n",
    "Do you see any patterns or interesting observations?\n",
    "- How many households are in most districts?\n",
    "- Where are most districts located (common latitude and longitude)? \n",
    "- Common median age? What are we really asking?\n",
    "- Common median house value? Outliers?\n",
    "- Common income range? Is it in dollars? What is max income?\n",
    "- Notice anything strange about housing median age or median house value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrBF3S4qt2lF"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15)) # calls matplotlib.pyplot.hist to plot histograms\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nuNRvj4t2lI"
   },
   "source": [
    "# Create a test set\n",
    "\n",
    "Before we do any further examination, we need to split our data set into *training* and *testing* sets. We will then set our test set aside and not look at it until we evaluate our ML system. \n",
    "\n",
    "If we wanted to be really proper, we would have split our data set before even looking at value counts, attribute statistics and  histograms. The less we know about the test set, the better. Otherwise, there is a temptation to \"teach to the exam\" when developing our ML approach. The problem with optimizing the approach for the test set, is that our ML system may perform well on instances in our test set, but may do poorly on future instances (i.e. it won't *generalize* well to new districts). \n",
    "\n",
    "As with the Iris data set, we will randomly assign instances (districts) to either the training or testing set. So that we get the same results when we rerun our code, we will set the random seed to the same value (42) each time. For this problem we will use a 80/20 split for training/testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBiDIbMAt2lJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q091EKfqt2lL"
   },
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PIYo9ZGiDYs"
   },
   "source": [
    "Curious. We see several entries with NaN for total bedrooms. \n",
    "\n",
    "*How many of these entries are there?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-IHCYVzgxlU"
   },
   "outputs": [],
   "source": [
    "test_set['total_bedrooms'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJ34DnT2iUov"
   },
   "source": [
    "How would we get just these NaN entries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQFZDHabhYD2"
   },
   "outputs": [],
   "source": [
    "null_frames = test_set[test_set['total_bedrooms'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5OVAcqNhfh_"
   },
   "outputs": [],
   "source": [
    "null_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vZyq-TCidgN"
   },
   "source": [
    "*Do we have a similar situation in the training set?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EPGe972inem"
   },
   "outputs": [],
   "source": [
    "train_set['total_bedrooms'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iN0F46XHiv8O"
   },
   "source": [
    "*What happens if we go back and resplit our data set, but this time use a different random seed (e.g. 12 instead of 42)?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrgqJpQ1t2lP"
   },
   "source": [
    "## Using stratified sampling\n",
    "\n",
    "When we randomly split our data set each instance is equally likely to end up in the test set. Treating each instance the same is appropriate in many cases.\n",
    "\n",
    "Suppose that, for our task, we chat with experts who say that median income for a district is very important for predicting the median home value. Seems reasonable. Families with more income are likely to have more expensive homes.\n",
    "\n",
    "Due to randomness, it's possible that median income distribution in our training set may be different than our test set. If the instances in our training set is not representative of the instances in our test set (and ultimately the real world), our ML system will perform poorly.\n",
    "\n",
    "Let's look at the current median income distribution among districts in our full data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xILyYg87t2lQ"
   },
   "outputs": [],
   "source": [
    "housing[\"median_income\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_VUD4cyt2lV"
   },
   "source": [
    "So most districts are not fabulously wealthy, even in California.\n",
    "\n",
    "When we split our data set into training and testing sets, we want to ensure that each median income level is equally represented in both sets. To do this, we use a sampling technique called *stratified sampling*. Our instances are grouped into subgroups called *strata*. Samples are randomly drawn from each stratum such that the training set is representative of the test set.\n",
    "\n",
    "## Creating the strata\n",
    "\n",
    "We want each stratum to have a reasonable number of instances. Small stratum are more likely to be over/under represented in the test set. For our data set, we will create 5 strata. Our stategy will be: \n",
    "1. Category 1 ranges from 0 to 1.5\n",
    "2. Category 2 ranges from 1.5 to 3\n",
    "3. Category 3 ranges from 3 to 4.5\n",
    "4. Category 3 ranges from 4.5 to 6\n",
    "5. Category 3 ranges from 6 to infinity.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bKjm7nBpPO9"
   },
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing['median_income'], \n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cv1zyp3kt2le"
   },
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdCiJ9K1t2lh"
   },
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-utQwH5Nt2lk"
   },
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9X3pMlot2lo"
   },
   "source": [
    "Not all strata are the same size, but each has a reasonable number of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKEsI7EEt2lo"
   },
   "source": [
    "## Performing the stratified sampling\n",
    "\n",
    "Scikit-Learn provides the class ``StratifiedShuffleSplit`` that will help us perform a split using our strata that we just created.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html\n",
    "\n",
    "This class can be used to create multiple different train/test splits. Here we only want one train/test split. If we were performing 10-fold crossvalidation (we'll get to that later), we would want 10 different train/test splits.\n",
    "\n",
    "The ``split()`` function iterates through each train/test split. So we use a for-loop to get the split even though there is only one split in this case. For each iteration, we get the indices of the instances for the training set (1D ndarray) and test set (1D ndarray).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBT3hOQEt2lp"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Create split object. Want 80% train, 20% test.\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split object is a generator. Need for loop to make it generate a split.\n",
    "# split(x, label) -> splits array x between train and test making sure that\n",
    "# each label (e.g. each income category) is equally represented in both train and test sets.\n",
    "# The probability distribution of the labels should be roughly the same in both sets.\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    print('Training samples: {}, testing samples: {}'\n",
    "          .format(train_index.shape, test_index.shape))\n",
    "    \n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtnj-hrgt2lt"
   },
   "source": [
    "## Verify stratified sampling\n",
    "\n",
    "We've split our data set. Now let's check to see if each strata is represented in the test set with the same probability of the overall data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jod0W7W6t2lu"
   },
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)  # stratified sampled test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYeEgOqIt2lx"
   },
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts() / len(housing)    # overall data set (both train and test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jghjFOqmt2lz"
   },
   "source": [
    "To see the effect of stratified sampling of our data set, let's compare our split to one without stratified sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sh5aU_U-t2l0"
   },
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "# regular, non-stratified sampling split\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "# create a data frame to show the probability of each strata \n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "\n",
    "# compute percentage each strata is over/under represented in the test sets\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ku3efe60t2l2"
   },
   "outputs": [],
   "source": [
    "compare_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXlViTi0q5bI"
   },
   "source": [
    "Note that the first income category is very small compared with the others.\n",
    "\n",
    "*What do you expect would happen after a stratified split if we adjusted the cuttoff for category 1 to go from 0 to 2.0 instead of 0 to 1.5?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7JrKFrEt2l4"
   },
   "source": [
    "The test set from the stratified sampling split is more representative of the overall data set than the non-stratified approach.\n",
    "\n",
    "Now that we have split our data, we can remove the \"income_cat\" column we created to define the strata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFpIQgFHt2l5"
   },
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1q0s6A9St2l6"
   },
   "source": [
    "# Discover and visualize the data to gain insights\n",
    "\n",
    "Now that we have set aside the test set, we can spend more time exploring the training set. Our exploration helps us develop an effective approach for solving the task.\n",
    "\n",
    "So we don't accidentally mess up our training set as we experiment and explore it, we will create a copy of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9W4nEt7Yt2l7"
   },
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp7tN9wNt2l-"
   },
   "source": [
    "## Visualizing geographic data\n",
    "\n",
    "Since our districts have latitude and longitude we can visualize the locations of the districts with a scatter plot (x-axis is longitude, y-axis is latitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vm74-sLEt2l-"
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zyYr2j5t2mB"
   },
   "source": [
    "Looks kind of like California. \n",
    "\n",
    "However, with the size of the data points its hard to see if there are data points really close together. \n",
    "\n",
    "If we set ``alpha``=0.1 (default is 1), then the dots are mostly transparent. This will show us if there are many districts really close together in some areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpRt3Np1t2mC"
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1WAU2vTt2mE"
   },
   "source": [
    "Now we can see that there are dense clusters of districts in a few areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6W-1484t2mF"
   },
   "source": [
    "## Visualize housing prices\n",
    "\n",
    "Let's add more information to our plot.\n",
    "- Make the size of the dot proportional to district population (option ``s``).\n",
    "- Set dot color (option ``c``) based on median house value. Blue = low value. Red = high value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gx099hNxulpP"
   },
   "source": [
    "The argument `sharex=False` fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https://github.com/pandas-dev/pandas/issues/10611). Thanks to Wilmer Arellano for pointing it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzEFMK5wt2mG"
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "    sharex=False)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3be-kdYYt2mO"
   },
   "source": [
    "## Looking for correlations\n",
    "\n",
    "Features are the input to machine learning systems. In theory, there is no such thing as a bad feature. Given enough training data, ML algorithms will learn to ignore unhelpful features. However, in practice, there is often too little data. So, it is nice if we can identify the most important features for our task. \n",
    "\n",
    "Identifying the most informative features can be difficult or impractical for some problems. One method is to compute correlation coefficients between each pair of attributes in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGLQFQJmt2mP"
   },
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr(numeric_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEaUC1u7sVcj"
   },
   "outputs": [],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_Uc4Ffvt2mS"
   },
   "source": [
    "For this problem, we are particularly interested in how much the attributes are correlated with median house value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmSANJu6t2mT"
   },
   "outputs": [],
   "source": [
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIqRMGTFt2mY"
   },
   "source": [
    "Correlation coefficients range between -1 and 1.\n",
    "- close to 1 implies a strong *positive* correlation between the attributes (i.e. house value goes *up* as the attribute goes up)\n",
    "- close to -1 implies a strong *negative* correlation between the attributes (i.e. house value goes *down* as the attribute goes up)\n",
    "- close to 0 implies little or no linear relationship between the attributes\n",
    "\n",
    "Note: Correlation coefficient only measures *linear* relationships between attributes, not *nonlinear* relationships.\n",
    "\n",
    "Which attribute has the strongest relationship with median house value?\n",
    "\n",
    "Which attribute has little correlation with median house value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k9GfTGQt2mZ"
   },
   "source": [
    "## Plotting attributes\n",
    "\n",
    "Another way to check for correlations between attribute pairs is to create scatter plots of attribute pairs. With 11 attributes in our data set, we would have $11^2 = 121$ plots. For the sake of this example, we will just consider a subset of 4 attributes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJy4kuHlt2ma"
   },
   "outputs": [],
   "source": [
    "# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "By6h9Kr8t2md"
   },
   "source": [
    "Each row has the same attribute along the y-axis. Each column has the same attribute along the x-axis.\n",
    "\n",
    "The diagonal plots show histograms for the row/column attribute.\n",
    "\n",
    "Once again, from these plots it seems that median income has the strongest correlation with median house value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbqR9k6vt2me"
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "             alpha=0.1)\n",
    "plt.axis([0, 16, 0, 550000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B274itzYt2mi"
   },
   "source": [
    "As we may have noticed when first looking at the attribute histograms, we see that median house value appears to top out at \\\\$500K. It is unlikely that this was the maximum value of a Californian house, even in 1990. More likely, when data was collected, there were boxes for house value ranges and the last box was \\$500K+. \n",
    "\n",
    "Is this a problem? Potentially. The system may not make accurate predictions for houses worth more than \\$500K. To deal with this we have a couple options:\n",
    "1. Go back and collect accurate median house values for this districts. This may not be possible since the data set was collected in 1990.\n",
    "2. Remove districts with median house value of \\$500K from training and test sets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_G3EqYdt2mj"
   },
   "source": [
    "# Experimenting with attribute combinations\n",
    "\n",
    "Another thing to look at before developing an ML system is combining attributes to create new, more informative features.\n",
    "\n",
    "Consider the \"total rooms\" attribute. This is the total number of rooms in a district. When thinking about the median value for a house, the total number of rooms in a district does not seem like it would be that informative. Our correlation coefficients and scatter plots support this intuition. \n",
    "\n",
    "However, if we consider the *average number of rooms per house* in a district, that  seems like it would have more correlation with median house price in a district. We can create a new attribute, \"rooms per household\", by combining our \"total room\" and \"households\" attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xT4PD0Vxt2mj"
   },
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XleOU7kMt2mm"
   },
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr(numeric_only=True)\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BstEbebkt2mr"
   },
   "source": [
    "Hmmm...while \"rooms per household\" has a stronger correlation with house price than \"total rooms\", it's not much stronger.\n",
    "\n",
    "*Why isn't it stronger?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eSjGv8Dt2ms"
   },
   "outputs": [],
   "source": [
    "corr_matrix[\"total_rooms\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqJfbJHOt2mx"
   },
   "source": [
    "*What do you see?*\n",
    "\n",
    "*What is the relationship between \"total rooms\" and \"households\", the two attributes used to compute \"rooms per household\"?*\n",
    "\n",
    "It is a common frustration to have a \"brilliant idea\" for a new feature and find out it does not improve performance of an ML system. Typically, we find in this case that the new feature does not add information that was not already available with the existing features. \n",
    "\n",
    "However, sometimes our feature ideas do provide additional information. Consider \"bedrooms per room\". This attribute has a stronger correlation than \"total bedrooms\". Hence, our feature engineering efforts can pay off at times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.\tHow many instances does the data set have? How many attributes originally?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.\tWhat is the importance of using stratified sampling?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.\tIn the example presented, which attribute was used in the stratified sampling process and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.\tHow many strata were used in the example? What was the range for each one?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.\tExplain in a few words how the stratified sampling works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.\tWhy is using correlations useful for gaining insights into data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.\tIn the example presented, which attribute showed the highest correlation with the median house value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.\tWhat is attribute combinations? How can this help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "L03_ML_project-exploring-data-complete.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
